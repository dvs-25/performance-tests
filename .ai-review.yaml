llm:
  provider: OLLAMA
  meta:
    model: qwen3-coder:480b-cloud # модель, которую будет использовать ревью
    max_tokens: 15000 # ограничение на размер ответа
    temperature: 0.3 # чем ниже, тем строже ответы
  http_client:
    timeout: 600 # запас по времени для генерации
    api_url: http://localhost:11434  # локальный endpoint Ollama

vcs:
  provider: GITHUB
  http_client:
    api_url: https://api.github.com
    timeout: 120

prompt:
  inline_prompt_files: [ ./prompts/inline.md ]
  context_prompt_files: [ ./prompts/inline.md ]
  summary_prompt_files: [ ./prompts/summary.md ]

review:
  dry_run: true